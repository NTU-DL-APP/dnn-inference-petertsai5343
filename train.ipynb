{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e1dd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用設備: cuda\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:34<00:00, 772kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 140kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.42M/4.42M [00:05<00:00, 811kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.15k/5.15k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "開始訓練...\n",
      "Epoch 1/15, Batch 0, Loss: 2.3072\n",
      "Epoch 1/15, Batch 100, Loss: 0.5921\n",
      "Epoch 1/15, Batch 200, Loss: 0.4152\n",
      "Epoch 1/15, Batch 300, Loss: 0.4072\n",
      "Epoch 1/15, Batch 400, Loss: 0.3342\n",
      "Epoch 1 完成 - 訓練準確率: 81.39%\n",
      "Epoch 2/15, Batch 0, Loss: 0.3779\n",
      "Epoch 2/15, Batch 100, Loss: 0.3367\n",
      "Epoch 2/15, Batch 200, Loss: 0.4231\n",
      "Epoch 2/15, Batch 300, Loss: 0.3607\n",
      "Epoch 2/15, Batch 400, Loss: 0.2709\n",
      "Epoch 2 完成 - 訓練準確率: 86.74%\n",
      "Epoch 3/15, Batch 0, Loss: 0.3141\n",
      "Epoch 3/15, Batch 100, Loss: 0.3698\n",
      "Epoch 3/15, Batch 200, Loss: 0.3601\n",
      "Epoch 3/15, Batch 300, Loss: 0.3006\n",
      "Epoch 3/15, Batch 400, Loss: 0.2130\n",
      "Epoch 3 完成 - 訓練準確率: 88.25%\n",
      "Epoch 4/15, Batch 0, Loss: 0.3137\n",
      "Epoch 4/15, Batch 100, Loss: 0.2961\n",
      "Epoch 4/15, Batch 200, Loss: 0.2286\n",
      "Epoch 4/15, Batch 300, Loss: 0.2736\n",
      "Epoch 4/15, Batch 400, Loss: 0.2318\n",
      "Epoch 4 完成 - 訓練準確率: 89.05%\n",
      "Epoch 5/15, Batch 0, Loss: 0.2652\n",
      "Epoch 5/15, Batch 100, Loss: 0.2713\n",
      "Epoch 5/15, Batch 200, Loss: 0.1918\n",
      "Epoch 5/15, Batch 300, Loss: 0.2137\n",
      "Epoch 5/15, Batch 400, Loss: 0.2405\n",
      "Epoch 5 完成 - 訓練準確率: 89.73%\n",
      "Epoch 6/15, Batch 0, Loss: 0.2485\n",
      "Epoch 6/15, Batch 100, Loss: 0.2222\n",
      "Epoch 6/15, Batch 200, Loss: 0.1416\n",
      "Epoch 6/15, Batch 300, Loss: 0.2205\n",
      "Epoch 6/15, Batch 400, Loss: 0.2407\n",
      "Epoch 6 完成 - 訓練準確率: 90.27%\n",
      "Epoch 7/15, Batch 0, Loss: 0.2267\n",
      "Epoch 7/15, Batch 100, Loss: 0.1447\n",
      "Epoch 7/15, Batch 200, Loss: 0.2299\n",
      "Epoch 7/15, Batch 300, Loss: 0.1967\n",
      "Epoch 7/15, Batch 400, Loss: 0.1825\n",
      "Epoch 7 完成 - 訓練準確率: 90.76%\n",
      "Epoch 8/15, Batch 0, Loss: 0.1932\n",
      "Epoch 8/15, Batch 100, Loss: 0.1634\n",
      "Epoch 8/15, Batch 200, Loss: 0.3243\n",
      "Epoch 8/15, Batch 300, Loss: 0.1333\n",
      "Epoch 8/15, Batch 400, Loss: 0.2062\n",
      "Epoch 8 完成 - 訓練準確率: 91.38%\n",
      "Epoch 9/15, Batch 0, Loss: 0.2299\n",
      "Epoch 9/15, Batch 100, Loss: 0.2719\n",
      "Epoch 9/15, Batch 200, Loss: 0.2397\n",
      "Epoch 9/15, Batch 300, Loss: 0.2295\n",
      "Epoch 9/15, Batch 400, Loss: 0.1735\n",
      "Epoch 9 完成 - 訓練準確率: 91.66%\n",
      "Epoch 10/15, Batch 0, Loss: 0.1776\n",
      "Epoch 10/15, Batch 100, Loss: 0.2290\n",
      "Epoch 10/15, Batch 200, Loss: 0.1900\n",
      "Epoch 10/15, Batch 300, Loss: 0.1587\n",
      "Epoch 10/15, Batch 400, Loss: 0.1800\n",
      "Epoch 10 完成 - 訓練準確率: 91.92%\n",
      "Epoch 11/15, Batch 0, Loss: 0.2239\n",
      "Epoch 11/15, Batch 100, Loss: 0.1521\n",
      "Epoch 11/15, Batch 200, Loss: 0.2664\n",
      "Epoch 11/15, Batch 300, Loss: 0.3397\n",
      "Epoch 11/15, Batch 400, Loss: 0.2156\n",
      "Epoch 11 完成 - 訓練準確率: 92.47%\n",
      "Epoch 12/15, Batch 0, Loss: 0.1813\n",
      "Epoch 12/15, Batch 100, Loss: 0.1582\n",
      "Epoch 12/15, Batch 200, Loss: 0.2349\n",
      "Epoch 12/15, Batch 300, Loss: 0.1396\n",
      "Epoch 12/15, Batch 400, Loss: 0.1908\n",
      "Epoch 12 完成 - 訓練準確率: 92.73%\n",
      "Epoch 13/15, Batch 0, Loss: 0.1599\n",
      "Epoch 13/15, Batch 100, Loss: 0.1583\n",
      "Epoch 13/15, Batch 200, Loss: 0.1608\n",
      "Epoch 13/15, Batch 300, Loss: 0.2373\n",
      "Epoch 13/15, Batch 400, Loss: 0.1504\n",
      "Epoch 13 完成 - 訓練準確率: 93.01%\n",
      "Epoch 14/15, Batch 0, Loss: 0.2521\n",
      "Epoch 14/15, Batch 100, Loss: 0.1194\n",
      "Epoch 14/15, Batch 200, Loss: 0.1658\n",
      "Epoch 14/15, Batch 300, Loss: 0.1311\n",
      "Epoch 14/15, Batch 400, Loss: 0.1638\n",
      "Epoch 14 完成 - 訓練準確率: 93.60%\n",
      "Epoch 15/15, Batch 0, Loss: 0.1213\n",
      "Epoch 15/15, Batch 100, Loss: 0.2287\n",
      "Epoch 15/15, Batch 200, Loss: 0.1350\n",
      "Epoch 15/15, Batch 300, Loss: 0.0980\n",
      "Epoch 15/15, Batch 400, Loss: 0.1702\n",
      "Epoch 15 完成 - 訓練準確率: 93.76%\n",
      "測試準確率: 88.58%\n",
      "PyTorch模型已儲存，測試準確率: 88.58%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# 檢查GPU可用性\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用設備: {device}')\n",
    "\n",
    "# 定義神經網路模型（只使用Dense、ReLU、Softmax層）\n",
    "class FashionMNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTNet, self).__init__()\n",
    "        # 只使用全連接層，符合專案要求\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 256)  # 28*28 = 784\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)    # 10個類別\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # 不在這裡套用softmax，訓練時用CrossEntropyLoss\n",
    "        return x\n",
    "\n",
    "# 資料預處理\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,))  # Fashion-MNIST的標準化參數\n",
    "])\n",
    "\n",
    "# 載入資料集\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 建立資料載入器\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "model = FashionMNISTNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 訓練函數\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=15):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}, '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "        \n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1} 完成 - 訓練準確率: {epoch_acc:.2f}%')\n",
    "\n",
    "# 測試函數\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'測試準確率: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# 執行訓練\n",
    "print(\"開始訓練...\")\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=15)\n",
    "\n",
    "# 測試模型\n",
    "accuracy = test_model(model, test_loader)\n",
    "\n",
    "# 儲存模型\n",
    "torch.save(model.state_dict(), 'fashion_mnist_pytorch.pth')\n",
    "torch.save(model, 'fashion_mnist_complete.pth')\n",
    "print(f\"PyTorch模型已儲存，測試準確率: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a9a1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用設備: cuda\n",
      "開始訓練進階模型...\n",
      "============================================================\n",
      "Epoch  1/60 | Train Loss: 2.2811 | Train Acc: 44.16% | Test Loss: 1.1296 | Test Acc: 70.72% | LR: 0.000400 | Time: 30.8s\n",
      "Epoch  2/60 | Train Loss: 1.3899 | Train Acc: 62.00% | Test Loss: 1.0573 | Test Acc: 74.40% | LR: 0.000400 | Time: 22.3s\n",
      "Epoch  3/60 | Train Loss: 1.2774 | Train Acc: 66.87% | Test Loss: 1.0174 | Test Acc: 75.23% | LR: 0.000400 | Time: 22.9s\n",
      "Epoch  4/60 | Train Loss: 1.2211 | Train Acc: 69.20% | Test Loss: 0.9869 | Test Acc: 77.14% | LR: 0.000400 | Time: 22.9s\n",
      "Epoch  5/60 | Train Loss: 1.1855 | Train Acc: 70.86% | Test Loss: 0.9715 | Test Acc: 76.74% | LR: 0.000400 | Time: 22.8s\n",
      "Epoch  6/60 | Train Loss: 1.1541 | Train Acc: 71.95% | Test Loss: 0.9611 | Test Acc: 77.86% | LR: 0.000400 | Time: 25.5s\n",
      "Epoch  7/60 | Train Loss: 1.1315 | Train Acc: 73.08% | Test Loss: 0.9343 | Test Acc: 79.44% | LR: 0.000400 | Time: 21.8s\n",
      "Epoch  8/60 | Train Loss: 1.1106 | Train Acc: 73.61% | Test Loss: 0.9316 | Test Acc: 79.58% | LR: 0.000400 | Time: 22.1s\n",
      "Epoch  9/60 | Train Loss: 1.0958 | Train Acc: 74.56% | Test Loss: 0.9198 | Test Acc: 80.20% | LR: 0.000400 | Time: 21.9s\n",
      "Epoch 10/60 | Train Loss: 1.0828 | Train Acc: 74.96% | Test Loss: 0.9127 | Test Acc: 80.76% | LR: 0.000400 | Time: 22.2s\n",
      "Epoch 11/60 | Train Loss: 1.0729 | Train Acc: 75.48% | Test Loss: 0.8942 | Test Acc: 81.53% | LR: 0.000400 | Time: 22.3s\n",
      "Epoch 12/60 | Train Loss: 1.0562 | Train Acc: 76.15% | Test Loss: 0.8875 | Test Acc: 81.63% | LR: 0.000400 | Time: 22.1s\n",
      "Epoch 13/60 | Train Loss: 1.0447 | Train Acc: 76.50% | Test Loss: 0.8840 | Test Acc: 81.68% | LR: 0.000400 | Time: 22.1s\n",
      "Epoch 14/60 | Train Loss: 1.0407 | Train Acc: 76.45% | Test Loss: 0.8880 | Test Acc: 81.79% | LR: 0.000400 | Time: 22.3s\n",
      "Epoch 15/60 | Train Loss: 1.0326 | Train Acc: 77.12% | Test Loss: 0.8758 | Test Acc: 82.67% | LR: 0.000400 | Time: 22.5s\n",
      "Epoch 16/60 | Train Loss: 1.0264 | Train Acc: 77.31% | Test Loss: 0.8763 | Test Acc: 82.62% | LR: 0.000400 | Time: 21.9s\n",
      "Epoch 17/60 | Train Loss: 1.0195 | Train Acc: 77.59% | Test Loss: 0.8624 | Test Acc: 82.81% | LR: 0.000400 | Time: 22.2s\n",
      "Epoch 18/60 | Train Loss: 1.0143 | Train Acc: 77.70% | Test Loss: 0.8616 | Test Acc: 83.15% | LR: 0.000400 | Time: 22.2s\n",
      "Epoch 19/60 | Train Loss: 1.0067 | Train Acc: 78.24% | Test Loss: 0.8573 | Test Acc: 83.09% | LR: 0.000400 | Time: 21.6s\n",
      "Epoch 20/60 | Train Loss: 1.0027 | Train Acc: 78.30% | Test Loss: 0.8506 | Test Acc: 83.49% | LR: 0.000400 | Time: 21.8s\n",
      "Epoch 21/60 | Train Loss: 0.9941 | Train Acc: 78.66% | Test Loss: 0.8506 | Test Acc: 83.33% | LR: 0.000400 | Time: 22.1s\n",
      "Epoch 22/60 | Train Loss: 0.9924 | Train Acc: 78.81% | Test Loss: 0.8480 | Test Acc: 83.53% | LR: 0.000400 | Time: 22.3s\n",
      "Epoch 23/60 | Train Loss: 0.9856 | Train Acc: 79.01% | Test Loss: 0.8457 | Test Acc: 83.67% | LR: 0.000400 | Time: 21.6s\n",
      "Epoch 24/60 | Train Loss: 0.9816 | Train Acc: 79.17% | Test Loss: 0.8418 | Test Acc: 83.69% | LR: 0.000400 | Time: 21.5s\n",
      "Epoch 25/60 | Train Loss: 0.9797 | Train Acc: 79.30% | Test Loss: 0.8377 | Test Acc: 83.67% | LR: 0.000400 | Time: 21.3s\n",
      "Epoch 26/60 | Train Loss: 0.9766 | Train Acc: 79.31% | Test Loss: 0.8325 | Test Acc: 84.23% | LR: 0.000400 | Time: 21.4s\n",
      "Epoch 27/60 | Train Loss: 0.9717 | Train Acc: 79.46% | Test Loss: 0.8303 | Test Acc: 84.27% | LR: 0.000400 | Time: 21.5s\n",
      "Epoch 28/60 | Train Loss: 0.9676 | Train Acc: 79.67% | Test Loss: 0.8315 | Test Acc: 83.98% | LR: 0.000400 | Time: 21.8s\n",
      "Epoch 29/60 | Train Loss: 0.9641 | Train Acc: 79.83% | Test Loss: 0.8270 | Test Acc: 84.55% | LR: 0.000400 | Time: 21.9s\n",
      "Epoch 30/60 | Train Loss: 0.9587 | Train Acc: 80.16% | Test Loss: 0.8259 | Test Acc: 84.50% | LR: 0.000400 | Time: 22.1s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 25652, 27272) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32md:\\miniconda\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32md:\\miniconda\\envs\\torch\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 230\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# 執行訓練\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 230\u001b[0m     model, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_advanced_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型已儲存，最終準確率: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 189\u001b[0m, in \u001b[0;36mtrain_advanced_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    186\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# 訓練\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# 測試\u001b[39;00m\n\u001b[0;32m    192\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_model(model, test_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[2], line 122\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m    119\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    120\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    123\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    125\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\miniconda\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32md:\\miniconda\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32md:\\miniconda\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 25652, 27272) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 檢查GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用設備: {device}')\n",
    "\n",
    "# 高性能模型架構\n",
    "class AdvancedFashionMNISTNet(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(AdvancedFashionMNISTNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # 更深更寬的網路架構\n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate * 0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate * 0.7)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate * 1.2)\n",
    "        \n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.dropout5 = nn.Dropout(dropout_rate * 0.8)\n",
    "        \n",
    "        self.fc6 = nn.Linear(64, 10)\n",
    "        \n",
    "        # 權重初始化\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        \n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "# 進階資料增強\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(15),                    # 隨機旋轉\n",
    "    transforms.RandomHorizontalFlip(0.5),            # 隨機水平翻轉\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # 仿射變換\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,))       # Fashion-MNIST標準化\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,))\n",
    "])\n",
    "\n",
    "# 載入資料集\n",
    "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.FashionMNIST('./data', train=False, transform=transform_test)\n",
    "\n",
    "# 資料載入器\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# 早停機制\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_acc = 0\n",
    "        \n",
    "    def __call__(self, val_acc):\n",
    "        if val_acc > self.best_acc + self.min_delta:\n",
    "            self.best_acc = val_acc\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            return self.counter >= self.patience\n",
    "\n",
    "# 訓練函數\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    \n",
    "    return running_loss / len(train_loader), 100 * correct / total\n",
    "\n",
    "# 測試函數\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    return test_loss / len(test_loader), 100 * correct / total\n",
    "\n",
    "# 主訓練流程\n",
    "def train_advanced_model():\n",
    "    model = AdvancedFashionMNISTNet(dropout_rate=0.3).to(device)\n",
    "    \n",
    "    # 損失函數和優化器\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # 標籤平滑\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    # 學習率調度器\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=0.01,\n",
    "        epochs=60,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=15, min_delta=0.001)\n",
    "    best_acc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"開始訓練進階模型...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(60):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 訓練\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # 測試\n",
    "        test_loss, test_acc = test_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        # 更新學習率\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 記錄最佳模型\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f'Epoch {epoch+1:2d}/60 | '\n",
    "              f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | '\n",
    "              f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}% | '\n",
    "              f'LR: {current_lr:.6f} | Time: {epoch_time:.1f}s')\n",
    "        \n",
    "        # 早停檢查\n",
    "        if early_stopping(test_acc):\n",
    "            print(f\"早停於第 {epoch+1} 輪，最佳準確率: {best_acc:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    # 載入最佳模型\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"訓練完成！最佳測試準確率: {best_acc:.2f}%\")\n",
    "    \n",
    "    # 儲存模型\n",
    "    torch.save(model.state_dict(), 'fashion_mnist_advanced.pth')\n",
    "    torch.save(model, 'fashion_mnist_complete_advanced.pth')\n",
    "    \n",
    "    return model, best_acc\n",
    "\n",
    "# 執行訓練\n",
    "if __name__ == \"__main__\":\n",
    "    model, accuracy = train_advanced_model()\n",
    "    print(f\"模型已儲存，最終準確率: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
